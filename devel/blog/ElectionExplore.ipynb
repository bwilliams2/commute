{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data from Postgis Database\n",
    "\n",
    "The data has already been loaded in to a Postgres database with the [Postgis](https://postgis.net/) extension to allow for geometric manipulations (covered in a previous post LINK).\n",
    "I mostly just want all of the table data to begin with. \n",
    "I could probably parse down what I am getting here based on location but it doesn't make a whole lot of difference right now.\n",
    "\n",
    "For the election precincts, I am adding the distance to representative latitude and longitude positions for Minneapolis and St. Paul as a lazy proxy for \"suburban\" and \"urban\" definitions to see if these are important considerations.\n",
    "This is something that will probably need to be revisited later.\n",
    "Postgis makes this very simple and uses a spheroid calculation with the 4326 projection.\n",
    "\n",
    "To calculate the distance from St. Paul to a given geometry, I use the `ST_Distance` function from Postgis which finds the geodesic distance between the provided point for St. Paul and the `geom_c` column of the election table which was calculated during database population.\n",
    "The `ST_Distance` function uses a spheroid distance caluclation when `geography` types are specified.\n",
    "\n",
    "```\n",
    "ST_Distance('SRID=4326;POINT(-93.2650 -44.9537)'::geography, e.geom_c)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2  # (if it is postgres/postgis)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../../.env\")\n",
    "con = psycopg2.connect(database=\"postgres\", user=\"postgres\", password=os.getenv(\"POSTGRES_PASSWORD\"), host=\"localhost\")\n",
    "\n",
    "stpaul = \"-93.0900 -44.9537\"\n",
    "minneapolis = \"-93.2650 44.9778\"\n",
    "sql = f\"select *, ST_Distance('SRID=4326;POINT({stpaul})'::geography, e.geom_c) as stp_dis, ST_Distance('SRID=4326;POINT({minneapolis})'::geography, e.geom_c) as mpls_dis from election e\"\n",
    "election_data = gpd.read_postgis(sql, con)\n",
    "sql = f\"select * from parcels\"\n",
    "org_parcel_data = gpd.read_postgis(sql, con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2012 uses a \"vtd\" column while every other year uses \"vtdid\" so quick fix for that \n",
    "election_data[\"vtdid\"] = election_data[\"vtdid\"].fillna(election_data[\"vtd\"]).drop(columns=\"vtd\")\n",
    "# Add city_dis column\n",
    "election_data[\"cit_dis\"] = election_data[[\"stp_dis\", \"mpls_dis\"]].min(axis=1)/1609.344 # meters to miles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limiting Parcels\n",
    "For now, I want to look at only \"single-family residental\" parcels.\n",
    "Analysis could be done with additional residental parcel categories, but there is a lot of missing info on the size of the buildings with condomimums and apartments that make it difficult to accurately analyze with these included.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_family = [\"100 Res 1 unit\", \"Res 1 unit\", \"100 Res 1 Unit\", \"Residential\", \"RESIDENTIAL SINGLE FAMILY\", 'RESIDENTIAL', 'Residential Lakeshore']\n",
    "res_mask = org_parcel_data[\"useclass1\"].isin(single_family)\n",
    "parcel_data = org_parcel_data.loc[res_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, limiting the information used from the parcels to total estimated value (`emv_total`), year built (`year_built`), and last sale date (`sale_date`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Precinct Statistics\n",
    "\n",
    "The parcel data is tagged by precinct for each year that geographic parcel information was available under columns coded as `vtdid_{YEAR}`.\n",
    "This"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"emv_total\", \"year_built\", \"sale_date\"] + [col for col in parcel_data.columns if \"vtd\" in col]\n",
    "parcel_data = parcel_data.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtd_cols = [col for col in parcel_data.columns if \"vtd\" in col]\n",
    "id_cols = [col for col in parcel_data.columns if \"vtd\" not in col]\n",
    "pivot_data = parcel_data.melt(id_vars=id_cols, value_vars = vtd_cols, var_name=\"year\", value_name=\"vtdid\")\n",
    "pivot_data[\"year\"] = pivot_data[\"year\"].str.strip(\"vtdid_\").astype(np.int)\n",
    "pivot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_precinct_stats(df):\n",
    "    df = df.loc[df[\"year\"] != 0 & df[\"year\"].notna()]\n",
    "    median_tax = df[\"emv_total\"].loc[df[\"emv_total\"] > 0].median()\n",
    "    mean_tax = df[\"emv_total\"].loc[df[\"emv_total\"] > 0].mean()\n",
    "    house_age = df.loc[:, \"year_built\"].loc[df[\"year_built\"] > 0].median()\n",
    "    mean_age = df.loc[:, \"year_built\"].loc[df[\"year_built\"] > 0].mean()\n",
    "    five_year_growth = (df.loc[df[\"year_built\"] >= 2015].shape[0])/df.shape[0]\n",
    "    return pd.Series({\"median_emv\": median_tax, \"mean_emv\": mean_tax, \"median_age\": house_age, \"mean_age\": mean_age, \"growth\": five_year_growth})\n",
    "\n",
    "processed = pivot_data.groupby([\"year\", \"vtdid\"]).apply(create_precinct_stats).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "election_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = [\"Anoka\", \"Dakota\", \"Hennepin\", \"Scott\", \"Ramsey\", \"Washington\", \"Carver\"]\n",
    "metro_mask = election_data[\"countyname\"].isin(counties)\n",
    "election_data.loc[metro_mask & (election_data.year == 2020)].plot(column=\"usprsdfl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_election = election_data.merge(processed, on=[\"year\", \"vtdid\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = set(election_data.loc[metro_mask].vtdid.unique())\n",
    "merged = set(merged_election.vtdid.unique())\n",
    "missing_vtdids = full.difference(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_vtdids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_election.loc[merged_election.countyname.isin(counties)].plot(column=\"usprsdfl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.GeoDataFrame(merged_election, geometry=\"geom\").plot(column=\"usprstotal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_margins(df):\n",
    "    vote_cols = [col for col in merged_election.columns if \"dfl\" in col]\n",
    "    for col in vote_cols:\n",
    "        base = col.replace(\"dfl\", \"\")\n",
    "        df[base+\"_margin\"] = (df[base+\"dfl\"].values - df[base+\"r\"].values)/df[base+\"total\"].values * 100\n",
    "        df[base+\"_vote_density\"] = df[base+\"total\"]/df[\"parcel_area\"]\n",
    "    return df\n",
    "margins_election = calc_margins(merged_election)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.GeoDataFrame(margins_election.loc[margins_election.year == 2020], geometry=\"geom\").plot(column=\"usprs_margin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert vtdid\n",
    "The VTDID numbering is not continuous.\n",
    "For exploration purposes I want to switch this to a continguous index.\n",
    "I aleady merged the data I need using VTDID so this isn't a problem to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(margins_election[\"vtdid\"].sort_values().astype(np.int).unique())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# margins_election[\"vtdid\"] = margins_election[\"vtdid\"].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(margins_election[\"vtdid\"].sort_values().unique())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presidential Data\n",
    "I will focus on the three presidential datasets included in the parcel election results from 2012, 2016, and 2020.\n",
    "The"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2012, 2016, 2020]\n",
    "pres_data = margins_election.loc[margins_election.year.isin(years)]\n",
    "fig, ax = plt.subplots(1, 2, figsize=[25,10])\n",
    "ax[0].plot(years, pres_data.groupby(\"year\")[\"usprstotal\"].sum(), \"--o\")\n",
    "ax[0].set_xlabel(\"year\")\n",
    "ax[0].set_ylabel(\"usprstotal\")\n",
    "sns.scatterplot(data=pres_data, x=\"vtdid\", y=\"usprstotal\", hue=\"year\",ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am especially interested in margins and margin changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at change in margins over years\n",
    "def calculate_margin_change(df):\n",
    "    row = df.iloc[-1]\n",
    "    data = df\n",
    "#     if df.shape[0] == 1:\n",
    "#         current_id = row[\"id\"]\n",
    "#         # Find intersection of other districts with unique district and mean years not in current data\n",
    "#         overlays = gpd.overlay(margins_election.loc[margins_election.id == current_id], margins_election.loc[(margins_election.id != current_id) & (margins_election.year != row[\"year\"])], how=\"intersection\")\n",
    "#         overlays = overlays.rename(columns={k: k.strip(\"_2\") for k in overlays.columns if \"_2\" in k}).drop(columns=[col for col in overlays.columns if \"_1\" in col]).dropna(how=\"all\", axis=1)\n",
    "#         data = overlays.groupby(\"year\").mean().reset_index().append(row, ignore_index=True)\n",
    "    \n",
    "    for beg, end in [[2012, 2016], [2016, 2020]]:#, [2012, 2020]]:\n",
    "        row_beg = data.loc[data.year == beg]\n",
    "        row_end = data.loc[data.year == end]\n",
    "        if len(row_beg) > 0 and len(row_end) > 0:\n",
    "            df[f\"{end}-{beg}\"] = row_end[\"usprs_margin\"].values[0] - row_beg[\"usprs_margin\"].values[0]\n",
    "    return df\n",
    "g = margins_election.groupby(\"vtdid\").apply(calculate_margin_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in g.columns:\n",
    "    try:\n",
    "        g[col] = pd.to_numeric(g[col])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# gpd.GeoDataFrame(g, geometry=\"geom\").to_file(\"FullStateProcessedElections.geojson\", driver='GeoJSON')\n",
    "\n",
    "# gpd.GeoDataFrame(g.loc[g.countyname.isin(counties)], geometry=\"geom\").to_file(\"ProcessedElections.geojson\", driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to Postgres\n",
    "At this point, I want to save the data back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con = psycopg2.connect(database=\"postgres\", user=\"postgres\", password=os.getenv(\"POSTGRES_PASSWORD\"), host=\"localhost\")\n",
    "# from sqlalchemy import create_engine\n",
    "# engine = create_engine(f\"postgresql+psycopg2://postgres:{os.getenv('POSTGRES_PASSWORD')}@localhost/postgres\")\n",
    "# gpd.GeoDataFrame(g, geometry=\"geom\").to_postgis(\"processed_election\", engine, index=False)\n",
    "# con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtd_values = g.loc[(g.year==2020) & (g.usprs_margin.isna())].vtd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.GeoDataFrame(g.loc[g.year == 2020], geometry=\"geom\").plot(column=\"usprs_margin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = margins_election[[\"mean_emv\", \"growth\", \"usprs_margin\", \"year\", \"usprstotal\", \"usprs_vote_density\", \"cit_dis\"]], hue='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "for year in [2020, 2016, 2012]:\n",
    "    d = margins_election.loc[margins_election[\"year\"] == year].dropna(axis=1, how=\"all\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=[15,10])\n",
    "\n",
    "    ax.set_title(year)\n",
    "    sns.scatterplot(\n",
    "        x=\"cit_dis\",\n",
    "        y=\"usprs_margin\",\n",
    "        hue=\"median_emv\",\n",
    "        size=\"growth\",\n",
    "        sizes=(10, 300),\n",
    "        palette=\"viridis\",\n",
    "        linewidth=0,\n",
    "        data=d\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Patterns in High Growth Areas\n",
    "From the regional parcel data, there is an encoding of `YEAR_BUILT` which would track well with population growth due to new single family housing developments.\n",
    "Again, I wish I had better fidelity on the effects of higher density housing because their are likely patterns there as well due to the growth of high density developments in outer ring suburbs in the Twin Cities metro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at mean of high growth areas over time\n",
    "def group_growth(val, limits=[0.10, 0.20]):\n",
    "    if val >= limits[-1]:\n",
    "        return \"High\"\n",
    "    elif val <= limits[0]:\n",
    "        return \"Low\"\n",
    "    else:\n",
    "        return \"Med\"\n",
    "    \n",
    "margins_election[\"growth_cat\"] = margins_election[\"growth\"].apply(lambda x: group_growth(x))\n",
    "#pd.cut(margins_election[\"growth\"], 3, labels=[\"Low\", \"Med\", \"High\"])\n",
    "margins_election.groupby([\"growth_cat\", \"year\"])[\"usprs_margin\"].describe().dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# growth = margins_election.groupby([\"year\", \"growth_cat\"])[\"usprs_margin\"].describe().dropna()\n",
    "# growth\n",
    "sns.boxplot(data=margins_election.loc[margins_election.year.isin(years)], x=\"year\", y=\"usprs_margin\", hue=\"growth_cat\", dodge=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=[25, 10])\n",
    "gdf = gpd.GeoDataFrame(margins_election.loc[(margins_election.year == 2020) & (margins_election.countyname.isin(counties))], geometry=\"geom\")\n",
    "gdf.crs = 4326\n",
    "gdf.plot(column=\"growth_cat\", ax=ax[1], legend=True)\n",
    "gdf.plot(column=\"usprs_margin\", ax=ax[0], legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimated value trends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at mean of high growth areas over time\n",
    "margins_election[\"median_emv_cat\"] = pd.cut(margins_election[\"median_emv\"], bins=[0, 150000, 250000, 450000, 600000, np.max(margins_election[\"median_emv\"])])\n",
    "#pd.cut(margins_election[\"growth\"], 3, labels=[\"Low\", \"Med\", \"High\"])\n",
    "margins_election.groupby([\"median_emv_cat\", \"year\"])[\"usprs_margin\"].describe().dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# growth = margins_election.groupby([\"year\", \"growth_cat\"])[\"usprs_margin\"].describe().dropna()\n",
    "# growth\n",
    "fig, ax = plt.subplots(figsize=[30,10])\n",
    "sns.boxplot(data=margins_election.loc[margins_election.year.isin(years)], hue=\"year\", y=\"usprs_margin\", x=\"median_emv_cat\", dodge=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=[25, 10])\n",
    "gdf = gpd.GeoDataFrame(margins_election.loc[(margins_election.year == 2020) & (margins_election.countyname.isin(counties))], geometry=\"geom\")\n",
    "gdf.crs = 4326\n",
    "gdf.plot(column=\"median_emv_cat\", ax=ax[1], legend=True)\n",
    "gdf.plot(column=\"usprs_margin\", ax=ax[0], legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance trends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at mean of high growth areas over time\n",
    "margins_election[\"cit_dis_cat\"] = pd.cut(margins_election[\"cit_dis\"], bins=[0, 5, 15, 20, 25, 30, np.max(margins_election[\"cit_dis\"])])\n",
    "#pd.cut(margins_election[\"growth\"], 3, labels=[\"Low\", \"Med\", \"High\"])\n",
    "margins_election.groupby([\"cit_dis_cat\", \"year\"])[\"usprs_margin\"].describe().dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# growth = margins_election.groupby([\"year\", \"growth_cat\"])[\"usprs_margin\"].describe().dropna()\n",
    "# growth\n",
    "fig, ax = plt.subplots(figsize=[30,10])\n",
    "sns.boxplot(data=margins_election.loc[margins_election.year.isin(years)], hue=\"year\", y=\"usprs_margin\", x=\"cit_dis_cat\", dodge=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=[25, 10])\n",
    "gdf = gpd.GeoDataFrame(margins_election.loc[(margins_election.year == 2020) & (margins_election.countyname.isin(counties))], geometry=\"geom\")\n",
    "gdf.crs = 4326\n",
    "gdf.plot(column=\"cit_dis_cat\", ax=ax[1], legend=True)\n",
    "gdf.plot(column=\"usprs_margin\", ax=ax[0], legend=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_cols = [col for col in g.columns if \"-20\" in col]\n",
    "id_cols = [col for col in g.columns if \"-20\" not in col]\n",
    "margin_change = g.melt(id_vars=id_cols, value_vars = margin_cols, var_name=\"timeframe\", value_name=\"margin_change\")\n",
    "margin_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[15,10])\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=\"median_emv\",\n",
    "    y=\"margin_change\",\n",
    "    size=\"growth\",\n",
    "    hue=\"timeframe\",\n",
    "#         palette=\"ch:r=-.2,d=.3_r\",\n",
    "    ax=ax,\n",
    "    linewidth=0,\n",
    "    data=margin_change\n",
    ")\n",
    "# sns.scatterplot(\n",
    "#     x=\"median_emv\",\n",
    "#     y=\"usprsdfl\",\n",
    "#     size=\"growth\",\n",
    "#     hue=\"timeframe\",\n",
    "# #         palette=\"ch:r=-.2,d=.3_r\",\n",
    "#     ax=ax2,\n",
    "#     linewidth=0,\n",
    "#     data=margin_change.loc[margin_change.year.isin([2012,2016,2020])]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "## PLS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "# X_cols = [\"median_emv\", \"median_age\", \"growth\", \"cit_dis\", \"usprs_vote_density\"]\n",
    "X_cols = [\"cit_dis\", \"growth\", \"usprs_vote_density\", \"median_emv\", \"median_age\"]\n",
    "Y_cols = [\"usprs_margin\"]\n",
    "# There is one that has a zero value\n",
    "\n",
    "year_mask = (margins_election.year == 2020) & (margins_election.countyname.isin(counties))\n",
    "input_data = margins_election.loc[year_mask]\n",
    "input_data=input_data.loc[input_data[\"median_emv\"] != 0]\n",
    "input_data = input_data.loc[~input_data[X_cols+Y_cols].isna().any(axis=1)]\n",
    "X = (input_data[X_cols] - input_data[X_cols].mean())/input_data[X_cols].std()\n",
    "Y = input_data[Y_cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls = PLSRegression(n_components=4)\n",
    "pls.fit(X_train, y_train)\n",
    "y_pred = pls.predict(X_test)\n",
    "fig, [ax, ax2] = plt.subplots(1,2, figsize=[25,10])\n",
    "# pred_data = pd.concat([X_test, y_test, pd.Series(np.squeeze(y_pred), name=\"predict\")], ignore_index=True)\n",
    "# pred_data\n",
    "sns.scatterplot(\n",
    "        x=y_test[\"usprs_margin\"],\n",
    "        y=np.squeeze(y_pred),\n",
    "        hue=X_test[\"median_emv\"],\n",
    "        size=X_test[\"growth\"],\n",
    "        sizes=(10, 300),\n",
    "        palette=\"viridis\",\n",
    "        linewidth=0,\n",
    "#         data=pred_data\n",
    "    ax=ax\n",
    "    )\n",
    "# sns.scatterplot(x=y_test, y=pls.predict(X_test), hue=\"\")\n",
    "sns.histplot(y_test[\"usprs_margin\"].values - np.squeeze(y_pred), ax=ax2)\n",
    "# sns.scatterplot(\n",
    "#         x=y_test[\"usprs_margin\"],\n",
    "#         y=y_test[\"usprs_margin\"].values - np.squeeze(y_pred),\n",
    "# #         hue=X_test[\"median_emv\"],\n",
    "# #         size=X_test[\"growth\"],\n",
    "#         sizes=(10, 300),\n",
    "#         palette=\"viridis\",\n",
    "#         linewidth=0,\n",
    "# #         data=pred_data\n",
    "#     ax=ax2\n",
    "#     )\n",
    "# ax2.plot(y_test - y_pred, \"o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Keras model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_cols = [\"cit_dis\", \"growth\", \"usprs_vote_density\", \"mean_emv\", \"median_emv\", \"median_age\", \"mean_age\"]\n",
    "Y_cols = [\"margin_change\"]\n",
    "# There is one that has a zero value\n",
    "\n",
    "\n",
    "year_mask = (margin_change.year == 2020) & (margin_change.timeframe == \"2020-2016\") & (margin_change[\"margin_change\"].notna())\n",
    "input_data = margin_change.loc[year_mask]\n",
    "input_data=input_data.loc[input_data[\"median_emv\"] != 0]\n",
    "input_data = input_data.loc[~input_data[X_cols+Y_cols].isna().any(axis=1)]\n",
    "\n",
    "# X = (input_data[X_cols] - input_data[X_cols].mean())/input_data[X_cols].std()\n",
    "X = input_data[X_cols]\n",
    "Y = input_data[Y_cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train.values.astype(np.float), y_train.values.astype(np.float)))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.values.astype(np.float), y_test.values.astype(np.float)))\n",
    "train_dataset = dataset.shuffle(len(input_data)).batch(1)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.LayerNormalization(),\n",
    "  tf.keras.layers.Dense(16, input_shape=(len(X_cols),)),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.MeanSquaredError(),\n",
    "    ]\n",
    ")\n",
    "model.fit(train_dataset, epochs=2000, verbose=0, validation_data=(X_test.values.astype(np.float), y_test.values.astype(np.float)), use_multiprocessing=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"CurrentFitAll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(X.values.astype(np.float)).numpy()\n",
    "d = plt.hist(Y.values - predictions, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(input_data, geometry=\"geom\")\n",
    "gdf[\"predict\"] = predictions.astype(np.float)\n",
    "gdf[\"predict_resd\"] = (Y.values - predictions).astype(np.float)\n",
    "gdf.plot(column=\"predict_resd\", legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[[\"margin_change\", \"predict_resd\"]].plot.scatter(x=\"margin_change\", y=\"predict_resd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(gdf.sort_values(\"predict_resd\")[X_cols + [\"predict_resd\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit with only Mean Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_cols = [\"cit_dis\", \"growth\", \"usprs_vote_density\", \"mean_emv\", \"mean_age\"]\n",
    "Y_cols = [\"margin_change\"]\n",
    "# There is one that has a zero value\n",
    "\n",
    "\n",
    "year_mask = (margin_change.year == 2020) & (margin_change.timeframe == \"2020-2016\") & (margin_change[\"margin_change\"].notna())\n",
    "input_data = margin_change.loc[year_mask]\n",
    "input_data=input_data.loc[input_data[\"median_emv\"] != 0]\n",
    "input_data = input_data.loc[~input_data[X_cols+Y_cols].isna().any(axis=1)]\n",
    "\n",
    "# X = (input_data[X_cols] - input_data[X_cols].mean())/input_data[X_cols].std()\n",
    "X = input_data[X_cols]\n",
    "Y = input_data[Y_cols]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train.values.astype(np.float), y_train.values.astype(np.float)))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.values.astype(np.float), y_test.values.astype(np.float)))\n",
    "train_dataset = dataset.shuffle(len(input_data)).batch(1)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.LayerNormalization(),\n",
    "  tf.keras.layers.Dense(16, input_shape=(len(X_cols),)),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(32, activation='relu'),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.MeanSquaredError(),\n",
    "    ]\n",
    ")\n",
    "model.fit(train_dataset, epochs=2000, verbose=0, validation_data=(X_test.values.astype(np.float), y_test.values.astype(np.float)), use_multiprocessing=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"CurrentFitMeanOnly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "d = keras.models.load_model(\"CurrentFitMeanOnly\")\n",
    "float(np.squeeze(d(np.array([[10.0, 0.1, 1000000.0, 200000.0, 2000.0]]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.values.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d(X.values.astype(np.float)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = model(X.values.astype(np.float)).numpy()\n",
    "d = plt.hist(Y.values - predictions, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(input_data, geometry=\"geom\")\n",
    "gdf[\"predict\"] = predictions.astype(np.float)\n",
    "gdf[\"predict_resd\"] = (Y.values - predictions).astype(np.float)\n",
    "gdf.plot(column=\"predict_resd\", legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[[\"margin_change\", \"predict_resd\"]].plot.scatter(x=\"margin_change\", y=\"predict_resd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(gdf.sort_values(\"predict_resd\")[X_cols + [\"predict_resd\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[[\"mean_age\", \"mean_emv\", \"cit_dis\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gdf.mean_age >= 0.8 * 1975) & (gdf.mean_age <= 1.5 * 1975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_matches(mean_age, mean_emv, cit_dis):\n",
    "    age_mask = (gdf.mean_age >= .8 * mean_age) & (gdf.mean_age <= 1.2 * mean_age)\n",
    "    emv_mask = (gdf.mean_emv >= .8 * mean_emv) & (gdf.mean_emv <= 1.2 * mean_emv)\n",
    "    cit_mask = (gdf.cit_dis >= .8 * cit_dis) & (gdf.cit_dis <= 1.2 * cit_dis)\n",
    "    return gdf.loc[(age_mask & emv_mask & cit_mask)]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_matches(1999, 450000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
